{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def get_movie_reviews(movie_url, max_reviews=1000):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    reviews = []\n",
        "    page_number = 1\n",
        "    while len(reviews) < max_reviews:\n",
        "        page_url = f'{movie_url}&page={page_number}'\n",
        "        response = requests.get(page_url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_divs = soup.find_all('div', class_='text show-more__control')\n",
        "        if not review_divs:\n",
        "            break  # No more reviews found\n",
        "        for review_div in review_divs:\n",
        "            if len(reviews) >= max_reviews:\n",
        "                break\n",
        "            reviews.append(review_div.text.strip())\n",
        "        page_number += 1\n",
        "    return reviews[:max_reviews]\n",
        "\n",
        "def save_reviews_to_csv(movie_name, reviews):\n",
        "    with open(f'{movie_name}_reviews.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Review'])\n",
        "        for review in reviews:\n",
        "            writer.writerow([review])\n",
        "\n",
        "def main():\n",
        "    # IMDb URL for the movie \"Dune\" released in 2023\n",
        "    movie_url = 'https://www.imdb.com/title/tt1160419/reviews?ref_=tt_ql_3'\n",
        "    movie_name = 'Dune'  # Movie name for file naming\n",
        "    reviews = get_movie_reviews(movie_url)\n",
        "    save_reviews_to_csv(movie_name, reviews)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize Spanish stopwords\n",
        "stop_words_spanish = set(stopwords.words('spanish'))\n",
        "\n",
        "# Initialize stemmer and lemmatizer for Spanish\n",
        "stemmer_spanish = SnowballStemmer('spanish')\n",
        "lemmatizer_spanish = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQhhVx9_fFyw",
        "outputId": "2391dfca-05b5-4115-c22b-a45ef0a983ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions for data cleaning\n",
        "Remove special characters and punctuations\n"
      ],
      "metadata": {
        "id": "F2qTfnbhibPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for data cleaning\n",
        "# Remove special characters and punctuations\n",
        "def remove_noise(text):\n",
        "    clean_text = re.sub(r'[^\\w\\sáéíóúü]', '', text)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "Aj0EgoutgFUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Numbers"
      ],
      "metadata": {
        "id": "_BJOlIjsiWOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "    clean_text = re.sub(r'\\d+', '', text)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "F-9CteWEgHAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stopwords"
      ],
      "metadata": {
        "id": "c0bWXJjQiP9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    clean_words = [word for word in words if word.lower() not in stop_words_spanish]\n",
        "    clean_text = ' '.join(clean_words)\n",
        "    return clean_text\n"
      ],
      "metadata": {
        "id": "X9S5-o4KgHcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting text to lowercase"
      ],
      "metadata": {
        "id": "QV83zrgMiEjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(text):\n",
        "    clean_text = text.lower()\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "V6LIYs0sgIKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performming Stemming"
      ],
      "metadata": {
        "id": "oN1TUH4Ih6ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "    words = text.split()\n",
        "    stemmed_words = [stemmer_spanish.stem(word) for word in words]\n",
        "    clean_text = ' '.join(stemmed_words)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "XfYpOatcgIwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Lemmatization"
      ],
      "metadata": {
        "id": "liNqyjxghc_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(text):\n",
        "    words = text.split()\n",
        "    lemmatized_words = [lemmatizer_spanish.lemmatize(word) for word in words]\n",
        "    clean_text = ' '.join(lemmatized_words)\n",
        "    return clean_text\n"
      ],
      "metadata": {
        "id": "3Hoj2bZshQCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b19b37-2ce5-4d49-c9aa-ac31a2bc35b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to 'Dune_cleaned_reviews.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('Dune_reviews.csv')\n",
        "\n",
        "# Apply each cleaning step and save the clean data in a new column\n",
        "df['Clean_Text'] = df['Review'].apply(remove_noise)\n",
        "df['Clean_Text'] = df['Clean_Text'].apply(remove_numbers)\n",
        "df['Clean_Text'] = df['Clean_Text'].apply(remove_stopwords)\n",
        "df['Clean_Text'] = df['Clean_Text'].apply(lowercase)\n",
        "df['Clean_Text'] = df['Clean_Text'].apply(stemming)\n",
        "df['Clean_Text'] = df['Clean_Text'].apply(lemmatization)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('Dune_cleaned_reviews.csv', index=False)\n",
        "\n",
        "print(\"Cleaned data saved to 'Dune_cleaned_reviews.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parts of speech (POS) Tagging"
      ],
      "metadata": {
        "id": "5gVuL1UO_HuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanfordnlp"
      ],
      "metadata": {
        "id": "gms8r8KyFTUH",
        "outputId": "e35cdb7f-f160-4708-b16c-35138c8ae713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (1.25.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.31.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554944c6-0575-4343-81eb-b46a2ae5516d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech (POS) Tagging:\n",
            "Noun count: 158105\n",
            "Verb count: 94333\n",
            "Adjective count: 56182\n",
            "Adverb count: 40711\n",
            "\n",
            "Constituency Parsing Trees:\n",
            "(S\n",
            "  (NP Review/NNP)\n",
            "  ,/,\n",
            "  (NP Clean_Text/NNP)\n",
            "  ''/''\n",
            "  (NP Denis/NNP Villeneuve/NNP)\n",
            "  has/VBZ\n",
            "  accomplished/VBN\n",
            "  what/WP\n",
            "  was/VBD\n",
            "  considered/VBN\n",
            "  (NP impossible/JJ)\n",
            "  (PP for/IN (NP decades/NNS))\n",
            "  ,/,\n",
            "  to/TO\n",
            "  write/VB\n",
            "  and/CC\n",
            "  direct/VB\n",
            "  (NP a/DT faithful/JJ adaptation/NN)\n",
            "  to/TO\n",
            "  (NP the/DT fantastic/JJ)\n",
            "  1965/CD\n",
            "  (NP sci-fi/JJ novel/NN)\n",
            "  (PP by/IN (NP Frank/NNP Herbert/NNP))\n",
            "  ./.)\n",
            "\n",
            "Named Entity Recognition:\n",
            "Person names: 9987\n",
            "Organizations: 3342\n",
            "Locations: 965\n",
            "Product names: 165\n",
            "Dates: 3018\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.parse import CoreNLPParser\n",
        "import spacy\n",
        "\n",
        "# Load the cleaned text data\n",
        "with open('/content/Dune_cleaned_reviews.csv', 'r', encoding='utf-8') as file:\n",
        "    cleaned_text = file.read()\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "adj_count = 0\n",
        "adv_count = 0\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    for word, tag in tagged_words:\n",
        "        if tag.startswith('N'):  # Noun\n",
        "            noun_count += 1\n",
        "        elif tag.startswith('V'):  # Verb\n",
        "            verb_count += 1\n",
        "        elif tag.startswith('J'):  # Adjective\n",
        "            adj_count += 1\n",
        "        elif tag.startswith('R'):  # Adverb\n",
        "            adv_count += 1\n",
        "\n",
        "print(\"Parts of Speech (POS) Tagging:\")\n",
        "print(\"Noun count:\", noun_count)\n",
        "print(\"Verb count:\", verb_count)\n",
        "print(\"Adjective count:\", adj_count)\n",
        "print(\"Adverb count:\", adv_count)\n",
        "\n",
        "# (2) Constituency Parsing (using NLTK)\n",
        "grammar = r\"\"\"\n",
        "    NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
        "    PP: {<IN><NP>}                # Chunk prepositions followed by NP\n",
        "    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments\n",
        "    CLAUSE: {<NP><VP>}            # Chunk NP, VP\n",
        "\"\"\"\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "\n",
        "print(\"\\nConstituency Parsing Trees:\")\n",
        "for sentence in sentences[:1]:  # Print trees for the first sentence as an example\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    tree = chunk_parser.parse(tagged_words)\n",
        "    print(tree)\n",
        "\n",
        "# (3) Named Entity Recognition (NER) using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "named_entities = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "    named_entities.extend([(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# Extract entities of interest\n",
        "person_names = [entity[0] for entity in named_entities if entity[1] == 'PERSON']\n",
        "organizations = [entity[0] for entity in named_entities if entity[1] == 'ORG']\n",
        "locations = [entity[0] for entity in named_entities if entity[1] == 'GPE']\n",
        "dates = [entity[0] for entity in named_entities if entity[1] == 'DATE']\n",
        "product_names = [entity[0] for entity in named_entities if entity[1] == 'PRODUCT']\n",
        "\n",
        "# Calculate counts of each entity\n",
        "person_count = len(person_names)\n",
        "organization_count = len(organizations)\n",
        "location_count = len(locations)\n",
        "product_count = len(product_names)\n",
        "date_count = len(dates)\n",
        "\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "print(\"Person names:\", person_count)\n",
        "print(\"Organizations:\", organization_count)\n",
        "print(\"Locations:\", location_count)\n",
        "print(\"Product names:\", product_count)\n",
        "print(\"Dates:\", date_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The assignment provided a comprehensive hands-on experience in natural language processing (NLP), covering data collection, cleaning, and various NLP tasks such as part-of-speech tagging, parsing, and named entity recognition. While challenging, especially in data collection and cleaning, the tasks were intellectually stimulating and rewarding, offering opportunities for exploration and problem-solving. The provided time to insufficient for me , depending on their prior experience and familiarity with NLP tools and techniques. Overall, the assignment offered valuable insights into NLP and practical application of skills in analyzing text data.\""
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}